{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast \n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import chardet\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import textblob\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from nltk import bigrams,trigrams,ngrams\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk import everygrams, word_tokenize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with business data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = pd.read_json('./yelp_dataset_2022/yelp_dataset_2022/business.json',lines = True)\n",
    "IL_business = business[business['state'] == 'CA']\n",
    "IL_business = IL_business.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(IL_business)):\n",
    "    cur_attr = IL_business['attributes'][i]\n",
    "    if bool(cur_attr) == False:\n",
    "        continue\n",
    "    for key in cur_attr.keys():\n",
    "        IL_business.loc[i,key] = cur_attr[key]\n",
    "\n",
    "\n",
    "def extract_info (df, feature):\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            cur_attr = df[feature][i]\n",
    "            cur_attr = ast.literal_eval(cur_attr)\n",
    "            for key in cur_attr.keys():\n",
    "                df.loc[i,key] = cur_attr[key]\n",
    "        except: \n",
    "            continue\n",
    "\n",
    "            \n",
    "def extract_info_hours (df, feature):\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            cur_attr = df[feature][i]\n",
    "            #cur_attr = ast.literal_eval(cur_attr)\n",
    "            for key in cur_attr.keys():\n",
    "                df.loc[i,key] = cur_attr[key]\n",
    "        except: \n",
    "            continue\n",
    "            \n",
    "extract_info(IL_business,'BusinessParking')    \n",
    "extract_info(IL_business,'GoodForMeal')     \n",
    "extract_info(IL_business,'Ambience')     \n",
    "extract_info_hours(IL_business,'hours')     \n",
    "     \n",
    "        \n",
    "IL_business = IL_business.drop(columns=['attributes','BusinessParking','GoodForMeal','Ambience'])\n",
    "IL_business = IL_business.rename({'stars':'average_stars'},axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Asian_keyword = ['Chinese','Korean','Japanese','Thai','Vietnamese','Asian','Sushi']\n",
    "Asian_list = []\n",
    "for i in range(len(IL_business)):\n",
    "    cur_cat = IL_business.loc[i,'categories']\n",
    "    try: \n",
    "        cur_cat = cur_cat.split(',')\n",
    "        if any(item in cur_cat for item in Asian_keyword):\n",
    "            Asian_list.append(IL_business.loc[i,'name'])\n",
    "    except: continue\n",
    "        \n",
    "IL_Asian_food = IL_business[IL_business.name.str.contains('|'.join(Asian_list))]\n",
    "#IL_Asian_food = IL_Asian_food.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn Monday-Sunday working hour to length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hour(time):\n",
    "    result = 0\n",
    "    if len(time) == 5:\n",
    "        if time[-2] == '00':\n",
    "            result = int(time[:-3])\n",
    "        elif int(time[-2]) > 15:\n",
    "            result = int(time[:-3])+1\n",
    "        else:\n",
    "            result = int(time[:-3])\n",
    "    elif len(time) == 4:\n",
    "        if time[-1] == '0':\n",
    "            result = int(time[:-2])\n",
    "        elif int(time[-1]) > 15:\n",
    "            result = int(time[:-2])+1\n",
    "        else:\n",
    "            result = int(time[:-2])\n",
    "    else:\n",
    "        if time[-1] == '0':\n",
    "            result = int(time[:-2])\n",
    "        elif int(time[-1]) > 15:\n",
    "            result = int(time[:-2])+1\n",
    "        else:\n",
    "            result = int(time[:-2])\n",
    "    return result\n",
    "            \n",
    "\n",
    "def calculate_total_hour(feature, new_feature):\n",
    "    for i in range(len(IL_Asian_food)):\n",
    "        try:\n",
    "            cur_schedule =  IL_Asian_food.loc[i,feature]\n",
    "            ls = cur_schedule.split('-')\n",
    "            if ls[0] == ls[1]:\n",
    "                IL_Asian_food.loc[i,new_feature] = 24\n",
    "            else:\n",
    "                work_hour = convert_hour(ls[1])-convert_hour(ls[0])\n",
    "                IL_Asian_food.loc[i,new_feature] =work_hour\n",
    "        except: \n",
    "            continue\n",
    "        \n",
    "\n",
    "calculate_total_hour('Monday','Monday_working_length')\n",
    "calculate_total_hour('Tuesday','Tuesday_working_length')\n",
    "calculate_total_hour('Wednesday','Wednesday_working_length')\n",
    "calculate_total_hour('Thursday','Thursday_working_length')\n",
    "calculate_total_hour('Friday','Friday_working_length')\n",
    "calculate_total_hour('Saturday','Saturday_working_length')\n",
    "calculate_total_hour('Sunday','Sunday_working_length')\n",
    "\n",
    "IL_Asian_food = IL_Asian_food.drop(columns=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and combine reivew with business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>comment_star</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
       "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
       "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
       "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
       "\n",
       "   comment_star                 date  \\\n",
       "0             3  2018-07-07 22:09:11   \n",
       "1             5  2012-01-03 15:28:18   \n",
       "2             3  2014-02-05 20:30:30   \n",
       "3             5  2015-01-04 00:01:03   \n",
       "4             4  2017-01-14 20:54:15   \n",
       "\n",
       "                                                text  \n",
       "0  If you decide to eat here, just be aware it is...  \n",
       "1  I've taken a lot of spin classes over the year...  \n",
       "2  Family diner. Had the buffet. Eclectic assortm...  \n",
       "3  Wow!  Yummy, different,  delicious.   Our favo...  \n",
       "4  Cute interior and owner (?) gave us tour of up...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv('review.csv')\n",
    "review = review.rename({'stars':'comment_star'},axis = 1)\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [IL_Asian_food, review]\n",
    "IL_Asian_business_review = pd.merge(review, IL_Asian_food,left_on='business_id',right_on = 'business_id', how = 'right' ).reset_index(drop=True)\n",
    "IL_Asian_business_review.to_csv('IL_Asian_business_review.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_open = IL_Asian_business_review[IL_Asian_business_review['is_open'] == 1]\n",
    "cur_closed = IL_Asian_business_review[IL_Asian_business_review['is_open'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (12,8))\n",
    "sns.countplot(IL_Asian_food['average_stars'])\n",
    "plt.xlabel('Average stars')\n",
    "plt.ylabel('Total count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_parking = IL_Asian_food[IL_Asian_food['lot'] == True]\n",
    "no_parking = IL_Asian_food[IL_Asian_food['lot'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_scale = [(0, 'orange'), (1,'red')]\n",
    "fig = px.scatter_mapbox(IL_Asian_food, \n",
    "                        lat=\"latitude\", \n",
    "                        lon=\"longitude\", \n",
    "                        hover_name=\"name\", \n",
    "                        hover_data=[\"average_stars\",'address'],\n",
    "                        color=\"lot\",\n",
    "                        color_continuous_scale=color_scale,\n",
    "                        size=\"average_stars\",\n",
    "                        zoom=8, \n",
    "                        height=800,\n",
    "                        width=800)\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color_scale = [(0, 'orange'), (1,'red')]\n",
    "\n",
    "fig = px.scatter_mapbox(IL_Asian_food, \n",
    "                        lat=\"latitude\", \n",
    "                        lon=\"longitude\", \n",
    "                        hover_name=\"name\", \n",
    "                        hover_data=[\"average_stars\"],\n",
    "                        color=\"average_stars\",\n",
    "                        color_continuous_scale=color_scale,\n",
    "                        size=\"average_stars\",\n",
    "                        zoom=8, \n",
    "                        height=800,\n",
    "                        width=800)\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IL_Asian_business_review['text'].head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IL_Asian_business_review['text_cleaned'].head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "lemm = WordNetLemmatizer()\n",
    "#stop = set(stopwords.words('english'))\n",
    "stop = stopwords.words('english')\n",
    "IL_Asian_business_review['text_word_list'] = IL_Asian_business_review['text'].str.split()\n",
    "IL_Asian_business_review['text_no_stopwords'] = IL_Asian_business_review['text_word_list'].apply(lambda x : [re.sub(r'[.,\"\\'-?:!;]', '', i) for i in x])\n",
    "IL_Asian_business_review['text_no_stopwords'] = IL_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'\\<a href', ' ', i) for i in x])\n",
    "IL_Asian_business_review['text_no_stopwords'] = IL_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'&amp;', '', i) for i in x])\n",
    "IL_Asian_business_review['text_no_stopwords'] = IL_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', i) for i in x])\n",
    "IL_Asian_business_review['text_no_stopwords'] = IL_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'<br />', ' ', i) for i in x])\n",
    "IL_Asian_business_review['text_no_stopwords'] = IL_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'\\'', ' ', i) for i in x])\n",
    "IL_Asian_business_review['text_no_stopwords'] = IL_Asian_business_review['text_no_stopwords'].apply(lambda x : [i for i in x if i not in stop])\n",
    "IL_Asian_business_review['text_no_stopwords'] = IL_Asian_business_review['text_no_stopwords'].apply(lambda x: [item.lower() for item in x])\n",
    "IL_Asian_business_review['text_cleaned'] = [' '.join(map(str,l)) for l in IL_Asian_business_review.text_no_stopwords]\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemm.lemmatize(word) for word in nltk.WordPunctTokenizer().tokenize(text)]\n",
    "IL_Asian_business_review['text_cleaned'] = IL_Asian_business_review.text_cleaned.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize each sentiment\n",
    "def categorize_sentiment(row):\n",
    "    if row['comment_star'] > 3:\n",
    "        return 'Positive'\n",
    "    elif row['comment_star'] == 3:\n",
    "        return 'Neutral'\n",
    "    elif row['comment_star'] < 3:\n",
    "        return 'Negative'\n",
    "IL_Asian_business_review['sentiment'] = IL_Asian_business_review.apply(lambda row : categorize_sentiment(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot word cloud\n",
    "def word_cloud(df, sentiment):\n",
    "    new_df= df[df['sentiment'] == sentiment].reset_index(drop=True)\n",
    "    wc = ''\n",
    "    for i in range(len(new_df)):\n",
    "        for word in new_df.loc[i,'text_cleaned']:\n",
    "            wc += ''.join(word)+' '\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                          background_color ='white',min_font_size = 10,\n",
    "                         collocation_threshold = 100, min_word_length=3).generate(wc)\n",
    "\n",
    "    plt.figure(figsize = (4, 4), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    # plt.title(sentiment)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    \n",
    "word_cloud(IL_Asian_business_review,'Positive')   \n",
    "word_cloud(IL_Asian_business_review,'Negative')\n",
    "word_cloud(IL_Asian_business_review,'Neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n_gram plot top-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(text,ngram):\n",
    "  temp=zip(*[text[i:] for i in range(0,ngram)])\n",
    "  ans=[' '.join(ngram) for ngram in temp]\n",
    "  return ans\n",
    "\n",
    "def run_ngram(training_set, gram_num):\n",
    "    #Add positive word and negative word count into their own dictionary respectivly\n",
    "    positive_dict = defaultdict(int)\n",
    "    negative_dict = defaultdict(int)\n",
    "\n",
    "    for text in training_set[training_set.sentiment == 'Positive'].text_cleaned:\n",
    "        for word in generate_N_grams(text,gram_num):\n",
    "            positive_dict[word] += 1\n",
    "\n",
    "    for text in training_set[training_set.sentiment == 'Negative'].text_cleaned:\n",
    "        for word in generate_N_grams(text,gram_num):\n",
    "            negative_dict[word] += 1\n",
    "\n",
    "    #Turn the two dictionary into dataframe and find the most frequent words\n",
    "    df_positive = pd.DataFrame(sorted(positive_dict.items(), key = lambda x : x[1], reverse= True))\n",
    "    df_negative = pd.DataFrame(sorted(negative_dict.items(), key = lambda x : x[1], reverse= True))\n",
    "\n",
    "    \n",
    "    #Bar plot for top ten positive words\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.subplots_adjust(top = 6, bottom=3,left = 5, right = 8)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.bar(df_positive[0][:10], df_positive[1][:10], color = 'green', width = 0.3)\n",
    "    if gram_num > 1:\n",
    "        plt.xticks(rotation = 45)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Top 10 positive\")\n",
    "    \n",
    "    \n",
    "    #Bar plot for top ten positive words\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.bar(df_negative[0][:10], df_negative[1][:10], color = 'red', width = 0.3)\n",
    "    if gram_num > 1:\n",
    "        plt.xticks(rotation = 45)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Top 10 negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ngram(IL_Asian_business_review,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ngram(IL_Asian_business_review,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,4))\n",
    "IL_Asian_business_review['text_new'] = [' '.join(map(str,l)) for l in IL_Asian_business_review.text_cleaned]\n",
    "transformed = tfidf.fit_transform(pd.Series(IL_Asian_business_review.text_new))\n",
    "\n",
    "tfidf_test_df = pd.DataFrame(transformed[0].T.todense(),index=tfidf.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "tfidf_test_df = tfidf_test_df.sort_values('TF-IDF', ascending=False)\n",
    "\n",
    "tfidf_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
