{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab982d5",
   "metadata": {},
   "source": [
    "# Import Packages & Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d193e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast \n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import chardet\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import textblob\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from nltk import bigrams,trigrams,ngrams\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk import everygrams, word_tokenize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "business = pd.read_json('./business.json',lines = True)\n",
    "CA_business = business[business['state'] == 'CA']\n",
    "CA_business = CA_business.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b085247",
   "metadata": {},
   "source": [
    "# Deal with business data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b62058",
   "metadata": {},
   "source": [
    "### Extract object like information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69493d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CA_business)):\n",
    "    cur_attr = CA_business['attributes'][i]\n",
    "    if bool(cur_attr) == False:\n",
    "        continue\n",
    "    for key in cur_attr.keys():\n",
    "        CA_business.loc[i,key] = cur_attr[key]\n",
    "\n",
    "\n",
    "def extract_info (df, feature):\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            cur_attr = df[feature][i]\n",
    "            cur_attr = ast.literal_eval(cur_attr)\n",
    "            for key in cur_attr.keys():\n",
    "                df.loc[i,key] = cur_attr[key]\n",
    "        except: \n",
    "            continue\n",
    "\n",
    "            \n",
    "# def extract_info_hours (df, feature):\n",
    "#     for i in range(len(df)):\n",
    "#         try:\n",
    "#             cur_attr = df[feature][i]\n",
    "#             #cur_attr = ast.literal_eval(cur_attr)\n",
    "#             for key in cur_attr.keys():\n",
    "#                 df.loc[i,key] = cur_attr[key]\n",
    "#         except: \n",
    "#             continue\n",
    "            \n",
    "def extract_object_info (df, feature):\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            cur_attr = df[feature][i]\n",
    "            #cur_attr = ast.literal_eval(cur_attr)\n",
    "            for key in cur_attr.keys():\n",
    "                df.loc[i,key] = cur_attr[key]\n",
    "        except: \n",
    "            continue           \n",
    "    \n",
    "            \n",
    "extract_info(CA_business,'BusinessParking')    \n",
    "extract_info(CA_business,'GoodForMeal')     \n",
    "extract_info(CA_business,'Ambience')     \n",
    "extract_object_info(CA_business,'hours')     \n",
    "extract_object_info(CA_business,'Music')     \n",
    "extract_object_info(CA_business,'BestNights')     \n",
    "        \n",
    "CA_business = CA_business.drop(columns=['attributes','BusinessParking','GoodForMeal','Ambience'])\n",
    "CA_business = CA_business.rename({'stars':'average_stars'},axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3706d",
   "metadata": {},
   "source": [
    "### Based on word in category and name, we extracted East Asian restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b05e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave non-NA categories and clean it up, then find those contain certain words\n",
    "temp_business = CA_business[CA_business['categories'].notna()].reset_index(drop = True)\n",
    "temp_business['category_cleaned'] = temp_business['categories'].str.split()\n",
    "temp_business['category_cleaned'] = temp_business['category_cleaned'].apply(lambda x : [re.sub(r'[.,\"\\'-?:!;]', '', i) for i in x])\n",
    "temp_business['category_cleaned'] = temp_business['category_cleaned'].apply(lambda x : [re.sub(r'\\<a href', ' ', i) for i in x])\n",
    "temp_business['category_cleaned'] = temp_business['category_cleaned'].apply(lambda x : [re.sub(r'&amp;', '', i) for i in x])\n",
    "temp_business['category_cleaned'] = temp_business['category_cleaned'].apply(lambda x : [re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', i) for i in x])\n",
    "temp_business['category_cleaned'] = temp_business['category_cleaned'].apply(lambda x : [re.sub(r'<br />', ' ', i) for i in x])\n",
    "temp_business['category_cleaned'] = temp_business['category_cleaned'].apply(lambda x : [re.sub(r'\\'', ' ', i) for i in x])\n",
    "temp_business['category_cleaned'] = temp_business['category_cleaned'].apply(lambda x: [item.lower() for item in x])\n",
    "\n",
    "\n",
    "Asian_keyword = ['Chinese','Korean','Japanese','Asian','Sushi','Taiwan','Taiwaness','Hongkong','noodles','dumplings']\n",
    "Asian_keyword = [name.lower() for name in Asian_keyword]\n",
    "Asian_business_id1 = []\n",
    "for i in range(len(temp_business)):\n",
    "    cur_cat = temp_business.loc[i,'category_cleaned']\n",
    "    try: \n",
    "        if any(item in cur_cat for item in Asian_keyword):\n",
    "            Asian_business_id1.append(temp_business.loc[i,'business_id'])\n",
    "    except: continue\n",
    "        \n",
    "        \n",
    "#Leave non-NA name and clean it up, then find those contain certain words\n",
    "temp_business = CA_business[CA_business['name'].notna()].reset_index(drop = True)\n",
    "temp_business['name_cleaned'] = temp_business['name'].str.split()\n",
    "temp_business['name_cleaned'] = temp_business['name_cleaned'].apply(lambda x : [re.sub(r'[.,\"\\'-?:!;]', '', i) for i in x])\n",
    "temp_business['name_cleaned'] = temp_business['name_cleaned'].apply(lambda x : [re.sub(r'\\<a href', ' ', i) for i in x])\n",
    "temp_business['name_cleaned'] = temp_business['name_cleaned'].apply(lambda x : [re.sub(r'&amp;', '', i) for i in x])\n",
    "temp_business['name_cleaned'] = temp_business['name_cleaned'].apply(lambda x : [re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', i) for i in x])\n",
    "temp_business['name_cleaned'] = temp_business['name_cleaned'].apply(lambda x : [re.sub(r'<br />', ' ', i) for i in x])\n",
    "temp_business['name_cleaned'] = temp_business['name_cleaned'].apply(lambda x : [re.sub(r'\\'', ' ', i) for i in x])\n",
    "temp_business['name_cleaned'] = temp_business['name_cleaned'].apply(lambda x: [item.lower() for item in x])\n",
    "\n",
    "\n",
    "Asian_keyword = ['Chinese','Korean','Japanese','Asian','Sushi','Taiwan','Taiwaness','Hongkong','noodles','dumplings']\n",
    "Asian_keyword = [name.lower() for name in Asian_keyword]\n",
    "Asian_business_id2 = []\n",
    "for i in range(len(temp_business)):\n",
    "    cur_cat = temp_business.loc[i,'name_cleaned']\n",
    "    try: \n",
    "        if any(item in cur_cat for item in Asian_keyword):\n",
    "            Asian_business_id2.append(temp_business.loc[i,'business_id'])\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37de2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracted Asian business\n",
    "Asian_business_id = set(Asian_business_id1+Asian_business_id2)\n",
    "CA_Asian_food = CA_business[CA_business['business_id'].isin(Asian_business_id)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d5eb80",
   "metadata": {},
   "source": [
    "### Remove records that have certain keyword that doesn't belong to what we are focusing on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811e2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records that has certain keywords that is related to other categories\n",
    "non_restaurant_list = ['Medicine', 'Health','Meical','Acupuncture','Massage','Vietnamese','Thai','Martial Arts','Italian','Grocery','Indonesian']\n",
    "non_restuarant_id = []\n",
    "for i in range(len(CA_Asian_food)):\n",
    "    cur_cat = CA_Asian_food.loc[i,'categories']\n",
    "    try: \n",
    "        if any(item in cur_cat for item in non_restaurant_list):\n",
    "            non_restuarant_id.append(CA_Asian_food.loc[i,'business_id'])\n",
    "    except: continue\n",
    "        \n",
    "CA_Asian_food = CA_Asian_food[~CA_Asian_food['business_id'].isin(non_restuarant_id)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b6d25",
   "metadata": {},
   "source": [
    "### Turn Monday - Sunday working hour to length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c5953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hour(time):\n",
    "    result = 0\n",
    "    if len(time) == 5:\n",
    "        if time[-2] == '00':\n",
    "            result = int(time[:-3])\n",
    "        elif int(time[-2]) > 15:\n",
    "            result = int(time[:-3])+1\n",
    "        else:\n",
    "            result = int(time[:-3])\n",
    "    elif len(time) == 4:\n",
    "        if time[-1] == '0':\n",
    "            result = int(time[:-2])\n",
    "        elif int(time[-1]) > 15:\n",
    "            result = int(time[:-2])+1\n",
    "        else:\n",
    "            result = int(time[:-2])\n",
    "    else:\n",
    "        if time[-1] == '0':\n",
    "            result = int(time[:-2])\n",
    "        elif int(time[-1]) > 15:\n",
    "            result = int(time[:-2])+1\n",
    "        else:\n",
    "            result = int(time[:-2])\n",
    "    return result\n",
    "\n",
    "def calculate_total_hour(feature, new_feature):\n",
    "    for i in range(len(CA_Asian_food)):\n",
    "        try:\n",
    "            cur_schedule =  CA_Asian_food.loc[i,feature]\n",
    "            ls = cur_schedule.split('-')\n",
    "            if ls[0] == ls[1]:\n",
    "                CA_Asian_food.loc[i,new_feature] = 24\n",
    "            else:\n",
    "                if(ls[1] == '0:0'):\n",
    "                    CA_Asian_food.loc[i,new_feature] = 24 - convert_hour(ls[0])\n",
    "                elif(convert_hour(ls[1]) < convert_hour(ls[0])):\n",
    "                    CA_Asian_food.loc[i,new_feature] = 24 + convert_hour(ls[1]) - convert_hour(ls[0])\n",
    "                else:\n",
    "                    work_hour = convert_hour(ls[1])-convert_hour(ls[0])\n",
    "                    CA_Asian_food.loc[i,new_feature] = work_hour\n",
    "        except: \n",
    "            continue\n",
    "        \n",
    "\n",
    "calculate_total_hour('Monday','Monday_working_length')\n",
    "calculate_total_hour('Tuesday','Tuesday_working_length')\n",
    "calculate_total_hour('Wednesday','Wednesday_working_length')\n",
    "calculate_total_hour('Thursday','Thursday_working_length')\n",
    "calculate_total_hour('Friday','Friday_working_length')\n",
    "calculate_total_hour('Saturday','Saturday_working_length')\n",
    "calculate_total_hour('Sunday','Sunday_working_length')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60edac7",
   "metadata": {},
   "source": [
    "### Based on the above cell of code, calculate weekly working hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e6f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualte weekly working hour\n",
    "CA_Asian_food['Total_hour'] = CA_Asian_food[['Monday_working_length','Tuesday_working_length',\n",
    "                                                                  'Wednesday_working_length','Thursday_working_length',\n",
    "                                                                  'Friday_working_length','Saturday_working_length',\n",
    "                                                                  'Sunday_working_length']].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a2eb7",
   "metadata": {},
   "source": [
    "### Categorize restaurant category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebfd8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chinese_word_list = ['Chinese','Bubble Tea','Szechuan','Dim Sum','Hot Pot']\n",
    "Japanese_word_list =['Japanese','Sushi','Sushi Bars','Ramen']\n",
    "Korean_word_list = ['Korean','Soul Food']\n",
    "Fusion_list = ['Asian Fusion','Fusion']\n",
    "\n",
    "#当同一个category里的词同时出现在两个及以上的list里面时算fusion\n",
    "for i in range(len(CA_Asian_food)):\n",
    "    cur_category = CA_Asian_food.loc[i,'categories'].split(',')\n",
    "    cur_category = [x.strip() for x in cur_category]\n",
    "    fusion_count = 0\n",
    "    C_count = 0\n",
    "    J_count = 0\n",
    "    K_count = 0\n",
    "    \n",
    "    if any(elem in Fusion_list for elem in cur_category):\n",
    "        CA_Asian_food.loc[i,'general_category'] = 'Asian Fusion'\n",
    "        continue\n",
    "    else:\n",
    "        if any(elem in Chinese_word_list for elem in cur_category):\n",
    "            fusion_count += 1\n",
    "            C_count += 1\n",
    "        if any(elem in Japanese_word_list for elem in cur_category):\n",
    "            fusion_count += 1\n",
    "            J_count +=1 \n",
    "        if any(elem in Korean_word_list for elem in cur_category):\n",
    "            fusion_count += 1\n",
    "            K_count += 1\n",
    "            \n",
    "    if fusion_count > 1:\n",
    "        CA_Asian_food.loc[i,'general_category'] = 'Asian Fusion'\n",
    "    else:\n",
    "        if C_count != 0:\n",
    "            CA_Asian_food.loc[i,'general_category'] = 'Chinese'\n",
    "        if J_count != 0:\n",
    "            CA_Asian_food.loc[i,'general_category'] = 'Japanese'\n",
    "        if K_count != 0:\n",
    "            CA_Asian_food.loc[i,'general_category'] = 'Korean'\n",
    "            \n",
    "CA_Asian_food = CA_Asian_food.drop(['categories'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8fe6a0",
   "metadata": {},
   "source": [
    "### Unifing feature type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382eadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alcohol_list = [\"u'none'\" ,\"'none'\",]\n",
    "alcohol_list = [ \"u'beer_and_wine'\" , \"'beer_and_wine'\" ,\"u'full_bar'\",\"'full_bar'\"]\n",
    "for i in range(len(CA_Asian_food)):\n",
    "    cur_alcohol = CA_Asian_food.loc[i,'Alcohol']\n",
    "    if cur_alcohol in non_alcohol_list:\n",
    "        CA_Asian_food.loc[i,'Alcohol'] = False\n",
    "    elif cur_alcohol in alcohol_list:\n",
    "        CA_Asian_food.loc[i,'Alcohol'] = True\n",
    "    else: \n",
    "        CA_Asian_food.loc[i,'Alcohol'] = None\n",
    "        \n",
    "wifi_list =  [\"u'free'\" ,\"'free'\"] \n",
    "no_wifi_list = [\"u'no'\", \"'no'\"]\n",
    "for i in range(len(CA_Asian_food)):\n",
    "    cur_wifi = CA_Asian_food.loc[i,'WiFi']\n",
    "    if cur_wifi in no_wifi_list:\n",
    "        CA_Asian_food.loc[i,'WiFi'] = False\n",
    "    elif cur_wifi in wifi_list:\n",
    "        CA_Asian_food.loc[i,'WiFi'] = True\n",
    "    else: \n",
    "        CA_Asian_food.loc[i,'WiFi'] = None\n",
    "        \n",
    "ok_noise =  [\"u'average'\", \"u'quiet'\",\"'average'\" ,\"'quiet'\"] \n",
    "not_ok_noise = [\"u'loud'\",\"'loud'\"]\n",
    "for i in range(len(CA_Asian_food)):\n",
    "    cur_noise = CA_Asian_food.loc[i,'NoiseLevel']\n",
    "    if cur_noise in ok_noise:\n",
    "        CA_Asian_food.loc[i,'acceptable_noise'] = True\n",
    "    elif cur_noise in not_ok_noise:\n",
    "        CA_Asian_food.loc[i,'acceptable_noise'] = False\n",
    "    else: \n",
    "        CA_Asian_food.loc[i,'acceptable_noise'] = None\n",
    "        \n",
    "CA_Asian_food = CA_Asian_food.drop(['NoiseLevel'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b1743",
   "metadata": {},
   "source": [
    "### Keep only opening restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc45d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_Asian_food = CA_Asian_food[CA_Asian_food['is_open'] != 0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dd27c",
   "metadata": {},
   "source": [
    "### Drop feature that will not be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "793ae70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_Asian_food = CA_Asian_food.drop(['Music','BestNights','Smoking','BYOBCorkage','BYOB','GoodForDancing','AgesAllowed'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8cd1a",
   "metadata": {},
   "source": [
    "# Combine Reivew dataset with business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8957a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv('./review.csv')\n",
    "review = review.rename({'stars':'comment_star'},axis = 1)\n",
    "frames = [CA_Asian_food, review]\n",
    "CA_Asian_business_review = pd.merge(review, CA_Asian_food,left_on='business_id',right_on = 'business_id', how = 'right' ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9736e98",
   "metadata": {},
   "source": [
    "### Categorize sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feacb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize each sentiment\n",
    "def categorize_sentiment(row):\n",
    "    if row['comment_star'] > 3:\n",
    "        return 'Positive'\n",
    "    elif row['comment_star'] == 3:\n",
    "        return 'Neutral'\n",
    "    elif row['comment_star'] < 3:\n",
    "        return 'Negative'\n",
    "CA_Asian_business_review['sentiment'] = CA_Asian_business_review.apply(lambda row : categorize_sentiment(row), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796ddbf",
   "metadata": {},
   "source": [
    "### Clean comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25e68a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append more words to stop word list based on word cloud\n",
    "stop = stopwords.words('english')\n",
    "stop.append('Santa Barbara')\n",
    "stop.append('santa barbara')\n",
    "stop.append('Santa')\n",
    "stop.append('Barbara')\n",
    "stop.append('santa')\n",
    "stop.append('barbara')\n",
    "stop.append('chinese restaurant')\n",
    "stop.append('korean bbq')\n",
    "stop.append('Korean bbq')\n",
    "stop.append('place')\n",
    "stop.append('u')\n",
    "stop.append('iv')\n",
    "stop.append('id')\n",
    "stop.append('i am')\n",
    "\n",
    "#Remove stopwords\n",
    "lemm = WordNetLemmatizer()\n",
    "#stop = set(stopwords.words('english'))\n",
    "\n",
    "CA_Asian_business_review['text_word_list'] = CA_Asian_business_review['text'].str.split()\n",
    "CA_Asian_business_review['text_no_stopwords'] = CA_Asian_business_review['text_word_list'].apply(lambda x : [re.sub(r'[.,\"\\'-?:!;]', '', i) for i in x])\n",
    "CA_Asian_business_review['text_no_stopwords'] = CA_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'\\<a href', ' ', i) for i in x])\n",
    "CA_Asian_business_review['text_no_stopwords'] = CA_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'&amp;', '', i) for i in x])\n",
    "CA_Asian_business_review['text_no_stopwords'] = CA_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', i) for i in x])\n",
    "CA_Asian_business_review['text_no_stopwords'] = CA_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'<br />', ' ', i) for i in x])\n",
    "CA_Asian_business_review['text_no_stopwords'] = CA_Asian_business_review['text_no_stopwords'].apply(lambda x : [re.sub(r'\\'', ' ', i) for i in x])\n",
    "CA_Asian_business_review['text_no_stopwords'] = CA_Asian_business_review['text_no_stopwords'].apply(lambda x: [item.lower() for item in x])\n",
    "CA_Asian_business_review['text_no_stopwords'] = CA_Asian_business_review['text_no_stopwords'].apply(lambda x : [i for i in x if i not in stop])\n",
    "CA_Asian_business_review['text_cleaned'] = [' '.join(map(str,l)) for l in CA_Asian_business_review['text_no_stopwords']]\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemm.lemmatize(word) for word in nltk.WordPunctTokenizer().tokenize(text)]\n",
    "CA_Asian_business_review['text_cleaned'] = CA_Asian_business_review.text_cleaned.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ff1c1",
   "metadata": {},
   "source": [
    "### Save to local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08edf6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_Asian_business_review.to_csv('CA_Asian_business_review.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad062611",
   "metadata": {},
   "source": [
    "# Extracted dataset for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d58d1",
   "metadata": {},
   "source": [
    "### Extracted features that we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "addbd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_feature_list = ['review_id', 'user_id', 'business_id', 'comment_star', 'date',\n",
    "      'name', 'city', 'average_stars', 'review_count',\n",
    "      'BusinessAcceptsCreditCards',\n",
    "       'BikeParking',  'RestaurantsTakeOut',\n",
    "       'RestaurantsReservations',\n",
    "       'RestaurantsTableService', 'GoodForKids', 'WheelchairAccessible',\n",
    "       'RestaurantsDelivery', 'HasTV', \n",
    "       'Alcohol', 'DogsAllowed', 'RestaurantsGoodForGroups',\n",
    "       'WiFi', 'BusinessAcceptsBitcoin', 'CoatCheck', 'DriveThru',\n",
    "       'RestaurantsCounterService',\n",
    "       'garage', 'street',\n",
    "       'lot', 'valet', 'dessert', 'latenight', 'lunch', 'dinner', 'brunch',\n",
    "       'breakfast', 'hipster', 'divey', 'intimate',\n",
    "       'trendy', 'upscale', 'classy', 'casual', 'Total_hour', 'acceptable_noise','general_category']\n",
    "ambiance_list = ['hipster', 'divey', 'intimate','trendy', 'upscale', 'classy', 'casual']\n",
    "modeling_df = CA_Asian_business_review[modeling_feature_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ee1a5",
   "metadata": {},
   "source": [
    "### Convert ambient feature to true, false and None (hipster,upscale, divey, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bee045b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(modeling_df)):\n",
    "    cur_hip =  modeling_df.loc[i,'hipster']\n",
    "    cur_trend = modeling_df.loc[i,'trendy']\n",
    "    if  (cur_hip == True) | (cur_trend == True):\n",
    "        modeling_df.loc[i,'hipster_trendy'] = True\n",
    "    elif ((cur_hip != True) & (cur_hip != False)) & ((cur_trend != True) & (cur_trend != False)):\n",
    "        modeling_df.loc[i,'hipster_trendy'] = None\n",
    "    else:\n",
    "         modeling_df.loc[i,'hipster_trendy'] = False\n",
    "            \n",
    "    cur_up = modeling_df.loc[i,'upscale']\n",
    "    cur_clas = modeling_df.loc[i,'classy']\n",
    "    if  (cur_up == True) | (cur_clas == True):\n",
    "        modeling_df.loc[i,'upscale_classy'] = True\n",
    "    elif ((cur_up != True) & (cur_up != False)) & ((cur_clas != True) & (cur_clas != False)):\n",
    "        modeling_df.loc[i,'upscale_classy'] = None\n",
    "    else:\n",
    "         modeling_df.loc[i,'upscale_classy'] = False\n",
    "            \n",
    "    cur_dive = modeling_df.loc[i,'divey']\n",
    "    if cur_dive == True:\n",
    "        modeling_df.loc[i,'divey_env'] = True\n",
    "    elif cur_dive == False:\n",
    "        modeling_df.loc[i,'divey_env'] = False\n",
    "    else:\n",
    "        modeling_df.loc[i,'divey_env'] = None\n",
    "    \n",
    "    cur_inti = modeling_df.loc[i,'intimate']\n",
    "    if cur_inti == True:\n",
    "        modeling_df.loc[i,'intimate_env'] = True\n",
    "    elif cur_inti == False:\n",
    "        modeling_df.loc[i,'intimate_env'] = False\n",
    "    else:\n",
    "        modeling_df.loc[i,'intimate_env'] = None\n",
    "        \n",
    "    cur_casual = modeling_df.loc[i,'casual']\n",
    "    if cur_casual == True:\n",
    "        modeling_df.loc[i,'casual_env'] = True\n",
    "    elif cur_casual == False:\n",
    "        modeling_df.loc[i,'casual_env'] = False\n",
    "    else:\n",
    "        modeling_df.loc[i,'casual_env'] = None\n",
    "\n",
    "modeling_df = modeling_df.drop(ambiance_list,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd7bed",
   "metadata": {},
   "source": [
    "### Convert other feautres to true, false and None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e6b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_feature_list = ['BusinessAcceptsCreditCards', 'BikeParking', 'RestaurantsTakeOut',\n",
    "        'RestaurantsReservations', 'RestaurantsTableService',\n",
    "       'GoodForKids', 'WheelchairAccessible', 'RestaurantsDelivery', 'HasTV',\n",
    "       'DogsAllowed', 'RestaurantsGoodForGroups',\n",
    "       'BusinessAcceptsBitcoin', 'CoatCheck', 'DriveThru',\n",
    "       'RestaurantsCounterService', 'garage', 'street', 'lot', 'valet',\n",
    "       'dessert', 'latenight', 'lunch', 'dinner', 'brunch', 'breakfast']\n",
    "\n",
    "\n",
    "\n",
    "for feature in other_feature_list:\n",
    "   for i in range(len(modeling_df)):\n",
    "    cur_cell = modeling_df.loc[i,feature]\n",
    "    if (cur_cell == 'True') | (cur_cell == True):\n",
    "        modeling_df.loc[i,feature] = True\n",
    "    elif (cur_cell == 'False') | (cur_cell == False):\n",
    "        modeling_df.loc[i,feature] = False\n",
    "    else:\n",
    "         modeling_df.loc[i,feature] = None\n",
    "            \n",
    "modeling_df = modeling_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468a40f",
   "metadata": {},
   "source": [
    "### Save to local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9d73e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.to_csv('modeling_df.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
